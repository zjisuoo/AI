## <강화학습의 목적>

- optimal reward 를 얻기 위해 agent 에게 optimal 한 behavior strategy 요구
- policy gradient 수식 $\pi$ = policy, $\theta$ = 특정 paramete, $\pi_\theta$ = 특정 parameter 구성된 함수
- bellman equation 에서 나오는 reward (objective) function 값은 policy 영향 받음
- 최적의 reward 를 얻기 위해 $\theta$ 를 최적화 함

---

## <베이즈 정리>

- 데이터 조건이 주어졌을 때 조건부 확률을 구하는 공식
- 데이터가 주어지기 전 사전 확률값 데이터가 주어지면 어떻게 변하는지 계산
- 데이터가 주어지기 전 이미 어느정도 확률값을 예측하고 있을 때
    
    이를 새로 수집하나 데이터와 합쳐 최종 결과에 반영
    
- 데이터 개수가 부족한 경우 아주 유용
    
    (데이터를 매일 추가적으로 얻는 상황에서 매일 전체 데이터를 대상으로 새로 분석작업을 할 필요 없이 
    
    오늘 들어온 데이터를 합쳐서 업데이트만 하면 됨)
    

### 베이즈 정리 확률 밀도 함수식

![전체 경우의 수 Y, y - 조건이 발생할 경우의 수 X, x](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-02_%25EC%2598%25A4%25EC%25A0%2584_11.26.21.png)

전체 경우의 수 Y, y - 조건이 발생할 경우의 수 X, x

![스크린샷 2023-03-03 오전 11.06.16.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-03_%25EC%2598%25A4%25EC%25A0%2584_11.06.16.png)

![total probability - y 교집합 x](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-03_%25EC%2598%25A4%25EC%25A0%2584_11.16.36.png)

total probability - y 교집합 x

## <가우시안 분포>

- 정규 분포 (normal distribution) 또는 가우스 분포 (Gaussian distribution)
- 연속 확률 분포의 한 종류로 수집 된 자료의 분포를 근사 하는데 주로 사용
- 중심 극한 정리에 의해 독립적인 확률변수들의 평균은 정규 분포에 가까워지는 성질 이용

- 가우시안 랜덤벡터 (Gaussian Random variables)
    - 랜덤 변수 X 에 대한 평균과 표준편차를 이용해 PDF 관한 식을 세울 수 있을 때,
        
        그 때의 식을 normal density function 이라고 하며 PDF 를 가우시안 밀도라 하며 
        
        이러한 랜덤 변수 X 가 가우시안 랜덤벡터
        
    - **랜덤 변수의 평균과 표준편차를 이용해 정규밀도함수 작성 가능**
    
    - 선형변환도 가우시안 랜덤벡터
        
        ![스크린샷 2023-03-03 오전 11.28.54.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-03_%25EC%2598%25A4%25EC%25A0%2584_11.28.54.png)
        
    - 랜덤벡터 X, Y 가 결합 가우시안 분포 일 때 X, Y 도 각각 가우시안 랜덤벡터
        
        ![스크린샷 2023-03-03 오전 11.33.21.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-03_%25EC%2598%25A4%25EC%25A0%2584_11.33.21.png)
        
    - 랜덤벡터 X, Y 가 결합 가우시안 분포 일 때 비상 단계이면 서로 독립
    - 랜덤벡터 X, Y 가 결합 가우시안 분포 일 때,
        
        X 조건부 확률 밀도 함수이면 Y 조건부 확률 밀도 함수도 가우시안
        
        ![스크린샷 2023-03-03 오전 11.55.34.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-03_%25EC%2598%25A4%25EC%25A0%2584_11.55.34.png)
        
        가우시안 로그-정책 확률 밀도함수 
        
        → A2C 알고리즘에서 actor 행동변수가 서로 독립인 가우시안이라 가정
        
        ![스크린샷 2023-03-03 오후 12.05.04.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-03_%25EC%2598%25A4%25ED%259B%2584_12.05.04.png)
        

---

## <마르코프 시퀀스>

- 현재 확률 정보가 주어진 상태에서 미래와 과거는 무관(=조건부 독립)한 랜덤 시퀀스
    
    → 확률 정보는 현재의 확률 정보에 녹아있다.
    
    ![스크린샷 2023-03-03 오후 12.19.39.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-03_%25EC%2598%25A4%25ED%259B%2584_12.19.39.png)
    

### 강화학습에서 에이전트와 환경 상호작용

![스크린샷 2023-03-03 오후 12.35.30.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-03_%25EC%2598%25A4%25ED%259B%2584_12.35.30.png)

1. agent 가 환경측정
2. agent 가 state 인허가
3. agent 행동에 의해 환경이 다음 상태로 전환
4. 전환된 환경 상태 반영하여 agent 새로운 행동 선택
5. 환경으로부터 보상 → 장기적 성과, 예측 통해 주기적 개선

### 강화학습 방법

환경 모델 추정(모델 기반 - 로봇 제어, 드론 제어) vs 가치 함수 추정(가치 기반 - DQN, actor-critic 구조)

모델가치 함수 추정 → 정책 개선 → 정책 실행 샘플 생성(정책파라미터 Q 계산목적) → 모델가치 함수 추정

---

## <벨만 최적 방정식>

### 최적 상태 가치

- 최적 상태 가치를 무한히 반복하여 계산하면 V(Xt) 값이 수렴하게 되고 정책 π 에 대한 상태 가치임
    
    ![스크린샷 2023-03-06 오후 5.32.43.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-06_%25EC%2598%25A4%25ED%259B%2584_5.32.43.png)
    
- 최적 정책을 찾아야 하기 때문에 정책 π 를 따라 갔을 때 받는 보상의 합인 가치 함수를 통해 판단해야함
    
    ![스크린샷 2023-03-06 오후 5.35.44.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-06_%25EC%2598%25A4%25ED%259B%2584_5.35.44.png)
    
- 최적 상태 가치가 최대 일 때 최적 정책을 정해야 함
    
    → 여러가지 정책 중 상태 가치를 최대로 만드는 정책  π 에서의 최적 상태 가치 함수 V(Xt)
    
    ![스크린샷 2023-03-06 오후 5.36.06.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-06_%25EC%2598%25A4%25ED%259B%2584_5.36.06.png)
    

### 최적 행동 가치

- 최적 상태 가치가 확정적일 때 행동 가치 함수에 대한 벨만 방정식
    
    → 아래 식이 벨만 최적 방정식(Bellman optimality equation)
    
    ![스크린샷 2023-03-06 오후 5.49.43.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-06_%25EC%2598%25A4%25ED%259B%2584_5.49.43.png)
    
    ![스크린샷 2023-03-06 오후 5.51.46.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-06_%25EC%2598%25A4%25ED%259B%2584_5.51.46.png)
    

### 최적 정책

- 최적 행동 가치 식은 현재의 최적 가치와 시간 스텝의 최적 상태 관계
    
    ![최적 상태 가치 함수 이용](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-06_%25EC%2598%25A4%25ED%259B%2584_5.58.39.png)
    
    최적 상태 가치 함수 이용
    
    ![스크린샷 2023-03-06 오후 5.59.46.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-06_%25EC%2598%25A4%25ED%259B%2584_5.59.46.png)
    

---

## Reinforce Algorithm-Monte Carlo

- 한 에피소드가 끝나야 정책 업데이트 가능(On-policy)
- gradient 분산이 매우 큼

### Workflow - Reinforce Algorithm

- episode(1)  $\pi_\theta(u_i|x_i)$ → $(T_1+1)$ 개 샘플
    - 샘플 생성
    - 반환값 계산(=object function gradient)
        
        ![스크린샷 2023-03-13 오후 3.46.20.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-13_%25EC%2598%25A4%25ED%259B%2584_3.46.20.png)
        
    - Loss function 계산
        
        ![스크린샷 2023-03-13 오후 3.48.23.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-13_%25EC%2598%25A4%25ED%259B%2584_3.48.23.png)
        
- update (episode (1) 샘플 폐기)
    
    ![스크린샷 2023-03-13 오후 3.49.42.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-13_%25EC%2598%25A4%25ED%259B%2584_3.49.42.png)
    
- episode(2)
- update (episode (2) 샘플 폐기)
- …

## A2C Algorithm

- 목적함수 최대 설정하여 항상 1인 교차 entropy 에 advantage 곱함 따라서 advantage 영향 많이 받음
- 정책과 asynchronous(On-policy)
- 가치 함수 학습 시 사용되는 샘플에 시간 상관

### Workflow-A2C

![정책 개선 = 배치 A2C/온라인 A2C → $u_i$ ~ $\pi_\theta(u_i|x_i)$](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-13_%25EC%2598%25A4%25ED%259B%2584_4.39.38.png)

정책 개선 = 배치 A2C/온라인 A2C → $u_i$ ~ $\pi_\theta(u_i|x_i)$

1. 정책으로 행동 확률적 선택
2. 정책 개선 
    
    2-1. 정책 실행하여 보상과 상태변수 측정하고 샘플 저장
    
    TD(Temporal Difference) Learning = 다음 step 미래추정가치 사용해 학습
    
    TD target = 보상 + value function 계산 
    
    ![스크린샷 2023-03-13 오후 4.45.18.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-13_%25EC%2598%25A4%25ED%259B%2584_4.45.18.png)
    
    2-2. 크리틱 신경망 손실함수 계산
    
    ![스크린샷 2023-03-13 오후 4.52.27.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-13_%25EC%2598%25A4%25ED%259B%2584_4.52.27.png)
    
    ![스크린샷 2023-03-13 오후 4.54.34.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-13_%25EC%2598%25A4%25ED%259B%2584_4.54.34.png)
    
    ![스크린샷 2023-03-13 오후 4.56.21.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-13_%25EC%2598%25A4%25ED%259B%2584_4.56.21.png)
    
     *2-2-1. actor 신경망 업데이트* 
    
    ![스크린샷 2023-03-13 오후 4.59.28.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-13_%25EC%2598%25A4%25ED%259B%2584_4.59.28.png)
    
    2-3. advantage 계산
    
    ![스크린샷 2023-03-13 오후 5.02.48.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-13_%25EC%2598%25A4%25ED%259B%2584_5.02.48.png)
    

→ 정책 개선하고 actor update 

![스크린샷 2023-03-13 오후 5.05.04.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-13_%25EC%2598%25A4%25ED%259B%2584_5.05.04.png)

### Actor class - A2C

state(3X1) → h1(64) →(ReLU) h2(32) →(ReLU) h3(16) →(ReLU) 평균값/표준편차 → Tanh/softplus

→ $\theta$ = 2898

### Critic class - A2C

state(3X1) → h1(64) →(ReLU) h2(32) →(ReLU) h3(16) →(ReLU) 상태가치

→ $\theta$ = 2881

## A3C Algorithm-Asynchronous advantage actor-critic

- Multi Thread 사용해 여러 하위 agent 비동기식 작동하여 에피소드 간 correlation 없음(On-policy)

### Workflow-A3C

![N 개 샘플 → $(x_i, u_i, r_i, x_{i+1})_N$](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-16_%25EC%2598%25A4%25EC%25A0%2584_10.47.30.png)

N 개 샘플 → $(x_i, u_i, r_i, x_{i+1})_N$

1. 워커의 정책 확률적으로 선택하고 샘플 저장
2. 워커의 n-step 시간차 target $y_{w,i}$ 계산
3. 워커의 n-step advantage 계산
    
    3-1. n-step 가치 추정(목적함수 gradient 계산 시 advantage 편향 줄임)
    

![스크린샷 2023-03-16 오전 11.03.07.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-16_%25EC%2598%25A4%25EC%25A0%2584_11.03.07.png)

 4. gradient 계산

4-1. 워커 critic 신경망 

![스크린샷 2023-03-16 오전 11.10.50.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-16_%25EC%2598%25A4%25EC%25A0%2584_11.10.50.png)

4-2. 워커 actor 신경망 

![스크린샷 2023-03-16 오전 11.11.27.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-16_%25EC%2598%25A4%25EC%25A0%2584_11.11.27.png)

1. global 신경망으로 워커 gradient 송부
    
    ![스크린샷 2023-03-16 오전 11.16.50.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-16_%25EC%2598%25A4%25EC%25A0%2584_11.16.50.png)
    
2. 업데이트 된 global 신경망 parameter 워커로 복사

 + sample 추가 

![스크린샷 2023-03-16 오전 11.22.30.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-16_%25EC%2598%25A4%25EC%25A0%2584_11.22.30.png)

### Actor class - A3C

state(3X1) → h1(64) →(ReLU) h2(32) →(ReLU) h3(16) →(ReLU) 평균값/표준편차 → Tanh/softplus

→ 로그 가우시안 정책

![스크린샷 2023-03-16 오후 11.47.15.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-16_%25EC%2598%25A4%25ED%259B%2584_11.47.15.png)

### Critic class - A3C

state(3X1) → h1(64) →(ReLU) h2(32) →(ReLU) h3(16) →(ReLU) 상태가치

## PPO Algorithm-Proximal Policy Optimization

- 근접 정책 최적화-정책 점진적 업데이트

### 목적함수

![스크린샷 2023-03-17 오후 11.29.42.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-17_%25EC%2598%25A4%25ED%259B%2584_11.29.42.png)

### surrogate

-최대화 위해 TRPO(trust region policy optimization) → $L(\theta)$ 선형화 2차함수로 근사한 후 최적값 구함

![스크린샷 2023-03-18 오전 12.03.04.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-18_%25EC%2598%25A4%25EC%25A0%2584_12.03.04.png)

-PPO 일정범위로로 한정위해 clipping

![스크린샷 2023-03-18 오전 12.19.08.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-18_%25EC%2598%25A4%25EC%25A0%2584_12.19.08.png)

![목적함수 일정수준 이상 커지는 것 제한](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-18_%25EC%2598%25A4%25EC%25A0%2584_12.19.50.png)

목적함수 일정수준 이상 커지는 것 제한

### gradient

![스크린샷 2023-03-18 오전 12.11.35.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-18_%25EC%2598%25A4%25EC%25A0%2584_12.11.35.png)

### Workflow-PPO

![N 개 샘플 → $(x_0, u_0, r_0, x_1)$ → 업데이트 → $(x_N, u_N, r_N, x_{N+1})$](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_2.09.21.png)

N 개 샘플 → $(x_0, u_0, r_0, x_1)$ → 업데이트 → $(x_N, u_N, r_N, x_{N+1})$

1. 이전 정책 gaussian 으로 가정하고 평균, 표준편차 계산
    
    확률 행동 선택 - 이전 정책 로그-확률밀도함수
    
    ![샘플 $x_1, x_2 … → x_i$](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_2.15.56.png)
    
    샘플 $x_1, x_2 … → x_i$
    
2. 데이터 모음 (정책, reward, advantage, 확률밀도함수값) batch 저장
    
    (정책 = $x_i,u_i$ / reward = $y_i$ / advantage = $A_i$ / 확률밀도함수값 = $log\pi_\theta{OLD}(u_i|x_i)$)
    
3. batch 크기만큼 데이터 추출
    
    3-1. critic 신경망 손실함수
    
    ![스크린샷 2023-03-20 오후 2.29.18.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_2.29.18.png)
    
    3-2. critic 신경망 업데이트
    
    ![스크린샷 2023-03-20 오후 2.32.31.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_2.32.31.png)
    
    3-3. 이전 정책, 현재 정책 비율 계산
    
    ![스크린샷 2023-03-20 오후 2.34.46.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_2.34.46.png)
    
4. surrogate gradient 계산하고 actor 신경망 업데이트
    
    ![스크린샷 2023-03-20 오후 2.40.33.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_2.40.33.png)
    
    ![3-3 비율 계산](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_2.41.05.png)
    
    3-3 비율 계산
    
    ![스크린샷 2023-03-20 오후 2.41.49.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_2.41.49.png)
    

### Actor class - PPO

state(3X1) → h1(64) →(ReLU) h2(32) →(ReLU) h3(16) →(ReLU) 평균값/표준편차 → Tanh/softplus

### Critic class - PPO

state(3X1) → h1(64) →(ReLU) h2(32) →(ReLU) h3(16) →(ReLU) 상태가치

→ 손실함수

![스크린샷 2023-03-20 오후 2.44.16.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_2.44.16.png)

### Agent class - GAE (역방향 시간의 궤환식)

![스크린샷 2023-03-20 오후 2.51.46.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_2.51.46.png)

![스크린샷 2023-03-20 오후 2.52.56.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_2.52.56.png)

![스크린샷 2023-03-20 오후 2.58.10.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_2.58.10.png)

![스크린샷 2023-03-20 오후 2.58.28.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_2.58.28.png)

→ $y_i = A^{GAE}_i+V_\phi(x_i)$

+ 어드밴티지 추정의 일반화 (GAE)

1-step 관계식 → 어드밴티지 추정값의 분산은 낮추고, 편향은 높임

![한개의 에피소드에서는 편향 X, 분산 증가-Monte Carlo](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_3.12.55.png)

한개의 에피소드에서는 편향 X, 분산 증가-Monte Carlo

n-step 관계식 → 1, 2 … 스텝 어드밴티지 계산한 뒤 가중 합산

![가중 합산](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_3.15.07.png)

가중 합산

![스크린샷 2023-03-20 오후 3.19.56.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-20_%25EC%2598%25A4%25ED%259B%2584_3.19.56.png)

## DDPG Algorithm-Deep Deterministic Policy Gradient

연속공간 행동변수 → 행동 자체 계산(replay buffer 에 저장하고 샘플 무작위로 꺼냄-experience replay)

- Off-policy 이전 정책 데이터 사용 가능
    - On-policy vs Off-policy
        - On-policy
            - 학습하는 policy 와 행동하는 policy 가 반드시 같아야만 학습 가능
            - 1번이라도 학습해서 업데이트 되면 이전 experience 모두 사용 불가
            - Importance sampling 등 일련의 과정을 거쳐야 재사용 가능
        - Off-policy
            - 학습하는 policy 와 행동하는 policy 가 반드시 같지 않아도 학습
            - 이전의 학습을 통해 얻은 experience 들도 새로운 학습에 사용 가능
- DQN(Deep Q-Network) 는 시간차 target 이 신경망 업데이트 할 때마다 계속 달라져 학습 불안정
    
    함수 근사화 + bootstrapping + Off policy 학습 방법 = Deadly Triad → 안전성 문제 
    
    - target actor network $(\theta’)$  , target critic network $(\phi’)$ 별도 운영
        
        ![스크린샷 2023-03-24 오전 11.45.51.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-24_%25EC%2598%25A4%25EC%25A0%2584_11.45.51.png)
        
    - target network 파라미터가 본 network 파라미터 느린 속도로 따라감
        
        ![스크린샷 2023-03-24 오전 11.48.45.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-24_%25EC%2598%25A4%25EC%25A0%2584_11.48.45.png)
        
        ![스크린샷 2023-03-24 오전 11.50.40.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-24_%25EC%2598%25A4%25EC%25A0%2584_11.50.40.png)
        
    - noise 도입하여 무작위성 추가
        
        $\pi_{noisy}(x_t)=\pi_\theta(x_t)+\varepsilon_t$ → Orientein-Uhlenbeck 노이즈
        
        $\varepsilon_{t+1}=\varepsilon_t+p(\mu-\varepsilon_t)\Delta t+ \sqrt{\Delta t}\sigma n_t$
        

### 목적함수

![스크린샷 2023-03-24 오전 11.15.03.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-24_%25EC%2598%25A4%25EC%25A0%2584_11.15.03.png)

### gradient

![스크린샷 2023-03-24 오전 11.23.03.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-24_%25EC%2598%25A4%25EC%25A0%2584_11.23.03.png)

### Actor 신경망 업데이트

![스크린샷 2023-03-24 오후 12.09.47.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-24_%25EC%2598%25A4%25ED%259B%2584_12.09.47.png)

### Workflow-DDPG

![N 개 샘플 → $x_i, u_i, r_i, x_{i+1}$](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-24_%25EC%2598%25A4%25ED%259B%2584_12.15.18.png)

N 개 샘플 → $x_i, u_i, r_i, x_{i+1}$

1. 랜덤 프로세스 초기화
2. 손실함수 이용해 critic 신경망 업데이트
    
    ![스크린샷 2023-03-24 오전 11.50.40.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-24_%25EC%2598%25A4%25EC%25A0%2584_11.50.40.png)
    
    2-1. gradient 식으로 actor 신경망 업데이트
    
    ![스크린샷 2023-03-24 오후 12.19.50.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-24_%25EC%2598%25A4%25ED%259B%2584_12.19.50.png)
    
    2-2. target critic, actor network 업데이트
    
    ![스크린샷 2023-03-24 오후 12.20.24.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-24_%25EC%2598%25A4%25ED%259B%2584_12.20.24.png)
    

### Actor class - DDPG

state(3X1) → h1(64) →(ReLU) h2(32) →(ReLU) h3(16) →(ReLU) 행동 → Tanh

### Critic class - DDPG

state(3X1) → x1(64) / 행동(1X1) →(ReLU) {h2(32) / a1(32)}=concatenate →(ReLU) h3(16) →(ReLU) 행동가치 → 손실함수

![스크린샷 2023-03-24 오후 12.24.40.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-24_%25EC%2598%25A4%25ED%259B%2584_12.24.40.png)

## SAC Algorithm-Soft Actor Critic

- 상황에 따라 차선 선택 가능 DDPG 접근 방식을 형성하여 비 정책 방식으로 확률적 정책 최적화
- 소프트벨만 iteration
- 소프트 행동가치 함수가 수렴할 때까지 정책 개선 번갈아 가면서 한 번씩 업데이트
- **엔트로피 정규화** - 정책의 무작위성 척도인 기대 수익과 엔트로피 간의 균형을 최대화하도록 훈련

### 목적함수

![정책 엔트로피 $H(\pi(u_t|x_t))$](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_2.58.34.png)

정책 엔트로피 $H(\pi(u_t|x_t))$

![스크린샷 2023-03-27 오후 3.04.25.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_3.04.25.png)

→ 최대 엔트로피 목적함수 (maximum entropy)

![$\alpha$ = 온도 파라미터 (temperature parameter)](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_3.11.48.png)

$\alpha$ = 온도 파라미터 (temperature parameter)

$\alpha$ 가 커지면 무작위성 높아짐 / **작아지면 정책확정성경향 올라감** 최적, 준최적 행동도 상황에 따라 선택 가능

### Soft Bellman Equation

- Soft state-value function = 상태 가치 함수 + entropy
    
    ![스크린샷 2023-03-27 오후 3.30.21.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_3.30.21.png)
    
- Soft action-value function = 표준 행동 가치 함수 + entropy
    
    ![$(\tau_{x_{t+1}:u_r}|u_r,u_t)$ → 상태변수 $x_t$ 에서 행동 $u_t$ 선택하고 어떤 정책 $\pi$ 로 생성되는 궤적](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_4.20.55.png)
    
    $(\tau_{x_{t+1}:u_r}|u_r,u_t)$ → 상태변수 $x_t$ 에서 행동 $u_t$ 선택하고 어떤 정책 $\pi$ 로 생성되는 궤적
    
- Soft state, Soft action 관계식
    
    ![$\alpha$ 가 0이면 소프트가치함수는 표준가치 함수로 환원 ](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_4.29.15.png)
    
    $\alpha$ 가 0이면 소프트가치함수는 표준가치 함수로 환원 
    
- 소프트 정책 개선 - 최대 엔트로피 목적함수를 greedy 정책 계산 (현재 시간만 고려하여 최대값 구함)
    - 최적 정책
        
        ![스크린샷 2023-03-27 오후 4.38.11.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_4.38.11.png)
        
        - Q-러닝, DDPG 최대 행동가치 노이즈 추가 인접행동가치 값으로 일정부분 확장하는 단일 모드 분포
        - 정책 행동가치가 지수값에 비례하기 때문에 다중모드 분포 → 가능성이 높은 오른 상태
    - 목적함수 greedy (argmax) 계산
        
        ![스크린샷 2023-03-27 오후 5.00.11.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_5.00.11.png)
        
    - Soft-action, KL 발산 최소화
        
        ![스크린샷 2023-03-27 오후 5.05.12.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_5.05.12.png)
        
        $exp(\frac{1}{\alpha}Q_{soft}^\pi(x_t, u_t))$ → 가우시안 분포 or 균등분포 or GMM (Gaussian Mixture Model)
        

### Workflow-SAC

![N 개 샘플 → $x_i, u_i, r_i, x_{i+1}$](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-24_%25EC%2598%25A4%25ED%259B%2584_12.15.18.png)

N 개 샘플 → $x_i, u_i, r_i, x_{i+1}$

1. transition sample 리플레이 버퍼 저장 → N 개 무작위 추출
    
    ![$u_{i+1}$ → $\pi_\theta$ 로 업데이트](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_5.13.36.png)
    
    $u_{i+1}$ → $\pi_\theta$ 로 업데이트
    
2. Q 신경망 업데이트 
    
    2-1. $q_i$ → on-policy → $u_i$ 에서서 샘플링 → $\phi ← \phi-\alpha_\phi\triangledown_\phi L_\phi(\phi)$
    
    ![$Q_{soft}(x_i,u_i)$ 에서 $q_i$ 로 업데이트](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_5.27.00.png)
    
    $Q_{soft}(x_i,u_i)$ 에서 $q_i$ 로 업데이트
    
    ![스크린샷 2023-03-27 오후 5.28.58.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_5.28.58.png)
    
    2-2. actor network 업데이트
    
    ![스크린샷 2023-03-27 오후 5.33.49.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_5.33.49.png)
    
    재파라미터화 트릭 (reparameterizaion trick)
    
    ![$\eta_j$ 노이즈 벡터](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_5.38.19.png)
    
    $\eta_j$ 노이즈 벡터
    
    2-3. target Q 신경망 업데이트
    
    $\phi’ ← \tau\phi+(1-\phi)\phi’$
    
- Q 신경망이 2개인 경우
    1. transition sample 리플레이 버퍼에 저장 → 2개 중 작은 값 추출
        
        ![스크린샷 2023-03-27 오후 5.43.26.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_5.43.26.png)
        
    2. Q 신경망 업데이트
        
        ![스크린샷 2023-03-27 오후 5.47.40.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-27_%25EC%2598%25A4%25ED%259B%2584_5.47.40.png)
        
        $\phi_1^{’} ← \tau\phi_1 + (1-\tau)\phi_1^{’}$
        
        $\phi_2^{’} ← \tau\phi_2 + (1-\tau)\phi_2^{’}$
        

### Actor class - SAC

state(3X1) → h1(64) →(ReLU) h2(32) →(ReLU) h3(16) →(ReLU) 평균/표준편차 → Tanh/softplus

### Critic class - SAC

state(3X1) → x1(64) / 행동(1X1) →(ReLU) {h2(32) / a1(32)}=concatenate →(ReLU) h3(16) →(ReLU) 행동가치

## 최적 제어-Optimal Control

### 상태공간 차분 방정식(State-space difference equation)

![비용함수 $C(x_t,u_t)=-r(x_t,u_t)$ 보상함수](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-29_%25EC%2598%25A4%25ED%259B%2584_1.04.12.png)

비용함수 $C(x_t,u_t)=-r(x_t,u_t)$ 보상함수

### dynamic programming - 벨만의 최적성 원리 이용한 최적화 방법

backward-in-time 최종 상태에서서 거슬러 올라감

![스크린샷 2023-03-29 오후 1.06.49.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-29_%25EC%2598%25A4%25ED%259B%2584_1.06.49.png)

### LQR(Linear Quadratic Regulator) - 최소 제어 크기로 상태변수 모두 0으로 수렴

![스크린샷 2023-03-29 오후 1.10.08.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-29_%25EC%2598%25A4%25ED%259B%2584_1.10.08.png)

- 역방향 패스 (칼만게인 계산)
    
    ![스크린샷 2023-03-29 오후 11.59.15.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-29_%25EC%2598%25A4%25ED%259B%2584_11.59.15.png)
    
- 순방향 패스 (최적 궤적 계산)
    
    ![스크린샷 2023-03-30 오전 12.02.38.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-30_%25EC%2598%25A4%25EC%25A0%2584_12.02.38.png)
    

### 확률적 LQR(stochastic LQR)

![스크린샷 2023-03-30 오전 10.15.38.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-30_%25EC%2598%25A4%25EC%25A0%2584_10.15.38.png)

![상수항 추가 → 프로세스 노이즈 공분산은 초기 상태변수 공분산과 무관](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-30_%25EC%2598%25A4%25EC%25A0%2584_10.16.47.png)

상수항 추가 → 프로세스 노이즈 공분산은 초기 상태변수 공분산과 무관

### 가우시안 LQR

확정적 정책 → $\pi(u_t|x_t)=N(x_t,\mu_t,S_t)$ 이 가우시안 확률밀도 함수 갖는 확률적 정책 확장 

(확정적 정책에서 $S_t$ 는 공분산)

$\pi(u_t|x_t)=NCK_t*x_t+k_t, Q_{uut}^{-1})$ ($Q_{uut}^{-1}$ 는 공분산)

### 반복적 LQR(iterative LQR, iLQR)

비선형 시스템에 LQR 적용

기존 궤적 = 명목 궤적, 실제 궤적 $x_t$ 를 명목 궤적 $\bar{x}_t$ 와 perturbation $\vartriangle{x_t}$ 합으로 표현

![스크린샷 2023-03-30 오전 10.28.48.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-30_%25EC%2598%25A4%25EC%25A0%2584_10.28.48.png)

![자코비안 행렬](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-30_%25EC%2598%25A4%25EC%25A0%2584_10.35.00.png)

자코비안 행렬

![선형화](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-30_%25EC%2598%25A4%25EC%25A0%2584_10.37.45.png)

선형화

![스크린샷 2023-03-30 오전 10.49.32.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-30_%25EC%2598%25A4%25EC%25A0%2584_10.49.32.png)

- 데이터 수집
- 로컬 모델 피팅(조건부 가우시안, GMM 사전분포, conjugate prior-normal inverse-Wishart)
- 로컬 제어 법칙 업데이트
    - Gaussian 공분산 업데이트 → 사전정보로 간주하고 공분산 $\mu_z^{MAP}, P_zz^{MAP}$ 업데이트
        
        ![스크린샷 2023-03-30 오전 10.58.41.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-30_%25EC%2598%25A4%25EC%25A0%2584_10.58.41.png)
        
- 최적화(dual gradient descent)
    
    ![스크린샷 2023-03-30 오전 11.02.16.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-30_%25EC%2598%25A4%25EC%25A0%2584_11.02.16.png)
    
    - 대체 비용 함수
        
        ![스크린샷 2023-03-30 오전 11.11.09.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-30_%25EC%2598%25A4%25EC%25A0%2584_11.11.09.png)
        
        ![스크린샷 2023-03-30 오전 11.11.32.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-30_%25EC%2598%25A4%25EC%25A0%2584_11.11.32.png)
        
    - 듀얼 변수
        
        ![스크린샷 2023-03-30 오전 11.15.15.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-30_%25EC%2598%25A4%25EC%25A0%2584_11.15.15.png)
        
        ![스크린샷 2023-03-30 오전 11.16.42.png](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%92%E1%85%AA%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%90%E1%85%B3%E1%86%A8%E1%84%85%E1%85%A9%E1%86%AB(3)%20827951ae5d5c43f0ba58223856e581cf/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-03-30_%25EC%2598%25A4%25EC%25A0%2584_11.16.42.png)
        
    - 업데이트 된 궤적 확률 밀도 사이 (KL 발산에 대한 한계값)